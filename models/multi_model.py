import torch
import torch.nn as nn
import torch.nn.functional as F
from models.eeg_models import EEGFeatureExtractor, EEGFusionNetwork
from utils.model_loader import ModelLoader
from models.image_models import ImageFeatureExtractor


class MultiModalFusionNetwork(nn.Module):
    def __init__(self,
                 eeg_model_name="EEGNetv1",
                 image_model_name="PlacesNet",
                 image_model_type="rsscnn",
                 in_chans=64,
                 n_classes=2,
                 input_window_samples=2000,
                 use_pretrained_eeg=True,
                 use_pretrained_image=True,
                 base_path="outputs",
                 common_dim=512,
                 private_dim=256,
                 dropout_rate=0.5,
                 alpha=0.5,  # å…¬å…±æŸå¤±æƒé‡
                 beta=0.5):  # ç§æœ‰æŸå¤±æƒé‡
        """
        æ”¹è¿›çš„å¤šæ¨¡æ€èåˆç½‘ç»œ - åŸºäºè„‘æœºè€¦åˆå­¦ä¹ æ€æƒ³

        Args:
            common_dim: å…¬å…±ç‰¹å¾ç»´åº¦
            private_dim: ç§æœ‰ç‰¹å¾ç»´åº¦
            alpha: å…¬å…±é€šé“ç›¸ä¼¼æ€§æŸå¤±æƒé‡
            beta: ç§æœ‰é€šé“å·®å¼‚æ€§æŸå¤±æƒé‡
        """
        super(MultiModalFusionNetwork, self).__init__()

        self.alpha = alpha
        self.beta = beta

        # åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨
        self.model_loader = ModelLoader(base_path)

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        pretrained_ssbcinet = None
        pretrained_image_model = None

        if use_pretrained_eeg:
            pretrained_ssbcinet = self.model_loader.load_eeg_model(
                model_name=eeg_model_name,
                in_chans=in_chans,
                n_classes=n_classes,
                input_window_samples=input_window_samples
            )

        if use_pretrained_image:
            pretrained_image_model = self.model_loader.load_image_model(
                model_type=image_model_type,
                base_model_name=image_model_name
            )

        # åˆå§‹åŒ–EEGç‰¹å¾æå–é€šè·¯
        self.eeg_feature_net = self._build_eeg_path(
            eeg_model_name=eeg_model_name,
            in_chans=in_chans,
            n_classes=n_classes,
            input_window_samples=input_window_samples,
            pretrained_ssbcinet=pretrained_ssbcinet
        )

        # åˆå§‹åŒ–å›¾åƒç‰¹å¾æå–é€šè·¯
        self.image_feature_net = self._build_image_path(
            image_model_name=image_model_name,
            pretrained_image_model=pretrained_image_model,
            image_model_type=image_model_type
        )

        # å…¬å…±é€šé“ç¼–ç å™¨ (å…±äº«å‚æ•°)
        self.common_encoder = nn.Sequential(
            nn.Linear(self.eeg_feature_net.out_dim, common_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(common_dim, common_dim),
            nn.ReLU()
        )

        # ç§æœ‰é€šé“ç¼–ç å™¨ (æ¯ä¸ªæ¨¡æ€ç‹¬ç«‹)
        self.eeg_private_encoder = nn.Sequential(
            nn.Linear(self.eeg_feature_net.out_dim, private_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(private_dim, private_dim),
            nn.ReLU()
        )

        self.image_private_encoder = nn.Sequential(
            nn.Linear(self.image_feature_net.out_dim, private_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(private_dim, private_dim),
            nn.ReLU()
        )

        # åˆ†ç±»å™¨
        fusion_dim = common_dim + private_dim * 2  # å…¬å…±ç‰¹å¾ + EEGç§æœ‰ç‰¹å¾ + å›¾åƒç§æœ‰ç‰¹å¾
        self.classifier = nn.Sequential(
            nn.Linear(fusion_dim, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate / 2),
            nn.Linear(128, n_classes if n_classes > 2 else 1)
        )

        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()

    def _build_eeg_path(self, eeg_model_name, in_chans, n_classes, input_window_samples, pretrained_ssbcinet):
        """æ„å»ºEEGç‰¹å¾æå–é€šè·¯"""

        class EEGFeaturePath(nn.Module):
            def __init__(self, feature_extractor, fusion_net, out_dim):
                super(EEGFeaturePath, self).__init__()
                self.feature_extractor = feature_extractor
                self.fusion = fusion_net
                self.out_dim = out_dim

            def forward(self, x1, x2):
                f1 = self.feature_extractor(x1)
                f2 = self.feature_extractor(x2)
                fused = self.fusion(f1, f2)
                return fused

        if pretrained_ssbcinet is not None:
            print("âœ… ä½¿ç”¨é¢„è®­ç»ƒçš„SSBCINetåˆå§‹åŒ–EEGé€šè·¯")
            feature_extractor = pretrained_ssbcinet.feature_extractor
            fusion_net = pretrained_ssbcinet.fusion
            out_dim = 512
            print("  âœ… æˆåŠŸåŠ è½½äº†è„‘ç”µé€šè·¯åˆå§‹åŒ–æƒé‡")
        else:
            print("ğŸ”„ éšæœºåˆå§‹åŒ–EEGé€šè·¯")
            feature_extractor = EEGFeatureExtractor(
                model_name=eeg_model_name,
                in_chans=in_chans,
                n_classes=n_classes,
                input_window_samples=input_window_samples,
            )
            fusion_net = EEGFusionNetwork(feature_extractor.out_dim)
            out_dim = 512

        return EEGFeaturePath(feature_extractor, fusion_net, out_dim)

    def _build_image_path(self, image_model_name, pretrained_image_model, image_model_type):
        """æ„å»ºå›¾åƒç‰¹å¾æå–é€šè·¯"""

        if pretrained_image_model is not None:
            print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒçš„{image_model_type.upper()}åˆå§‹åŒ–å›¾åƒé€šè·¯")
            return ImageFeatureExtractor(
                base_model_name=image_model_name,
                pretrained_rsscnn=pretrained_image_model
            )
        else:
            print("ğŸ”„ éšæœºåˆå§‹åŒ–å›¾åƒé€šè·¯")
            return ImageFeatureExtractor(
                base_model_name=image_model_name,
                pretrained_rsscnn=None
            )

    def forward(self, eeg1, eeg2, img1, img2):
        """
        å‰å‘ä¼ æ’­
        Returns:
            logits: åˆ†ç±»è¾“å‡º
            eeg_common: EEGå…¬å…±ç‰¹å¾
            image_common: å›¾åƒå…¬å…±ç‰¹å¾
            eeg_private: EEGç§æœ‰ç‰¹å¾
            image_private: å›¾åƒç§æœ‰ç‰¹å¾
        """
        # æå–åŸºç¡€ç‰¹å¾
        eeg_base_features = self.eeg_feature_net(eeg1, eeg2)
        image_base_features = self.image_feature_net(img1, img2)

        # å…¬å…±é€šé“ç‰¹å¾
        eeg_common = self.common_encoder(eeg_base_features)
        image_common = self.common_encoder(image_base_features)

        # ç§æœ‰é€šé“ç‰¹å¾
        eeg_private = self.eeg_private_encoder(eeg_base_features)
        image_private = self.image_private_encoder(image_base_features)

        # ç‰¹å¾èåˆ: å…¬å…±ç‰¹å¾ + EEGç§æœ‰ç‰¹å¾ + å›¾åƒç§æœ‰ç‰¹å¾
        fused_features = torch.cat([eeg_common, eeg_private, image_private], dim=1)

        # åˆ†ç±»
        logits = self.classifier(fused_features)

        if logits.shape[1] == 1:
            logits = logits.squeeze()  # äºŒåˆ†ç±»

        return logits, eeg_common, image_common, eeg_private, image_private

    def compute_loss(self, eeg_common, image_common, eeg_private, image_private, logits, targets):
        """
        è®¡ç®—æ€»æŸå¤±ï¼ŒåŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š
        1. åˆ†ç±»æŸå¤±
        2. å…¬å…±é€šé“ç›¸ä¼¼æ€§æŸå¤±
        3. ç§æœ‰é€šé“å·®å¼‚æ€§æŸå¤±
        """
        # 1. åˆ†ç±»æŸå¤±
        if logits.dim() == 1:  # äºŒåˆ†ç±»
            task_loss = F.binary_cross_entropy_with_logits(logits, targets.float())
        else:  # å¤šåˆ†ç±»
            task_loss = F.cross_entropy(logits, targets)

        # 2. å…¬å…±é€šé“ç›¸ä¼¼æ€§æŸå¤± (ä½¿ç”¨CMDè·ç¦»)
        common_sim_loss = self.cmd_loss(eeg_common, image_common, K=3)

        # 3. ç§æœ‰é€šé“å·®å¼‚æ€§æŸå¤±
        private_diff_loss = self.orthogonality_loss(eeg_common, eeg_private, image_common, image_private)

        # æ€»æŸå¤±
        total_loss = task_loss + self.alpha * common_sim_loss + self.beta * private_diff_loss

        return {
            'total_loss': total_loss,
            'task_loss': task_loss,
            'common_sim_loss': common_sim_loss,
            'private_diff_loss': private_diff_loss
        }

    def cmd_loss(self, X, Y, K=3):
        """
        ä¸­å¿ƒçŸ©å·®å¼‚æŸå¤± (Central Moment Discrepancy)
        è®ºæ–‡ä¸­ä½¿ç”¨çš„è·ç¦»åº¦é‡æ–¹æ³•
        """
        # ä¸€é˜¶çŸ© (å‡å€¼)
        x_mean = torch.mean(X, 0)
        y_mean = torch.mean(Y, 0)

        moment_diff = torch.norm(x_mean - y_mean, p=2)

        diffs = [moment_diff]  # ç´¯ç§¯æ‰€æœ‰ moment å·®å¼‚

        for k in range(2, K + 1):
            x_moment = torch.mean((X - x_mean) ** k, dim=0)
            y_moment = torch.mean((Y - y_mean) ** k, dim=0)
            diffs.append(torch.norm(x_moment - y_moment, p=2))

        return sum(diffs)

    def orthogonality_loss(self, eeg_common, eeg_private, image_common, image_private):
        """
        ç®€å•çš„æ­£äº¤æ€§æŸå¤± - ç›´æ¥å¤„ç†ç»´åº¦ä¸åŒ¹é…
        """

        def dimension_aware_loss(A, B):
            """ç»´åº¦æ„ŸçŸ¥çš„æŸå¤±è®¡ç®—"""
            # ç»Ÿä¸€åˆ°æœ€å°ç»´åº¦
            min_dim = min(A.size(1), B.size(1))
            A_trim = A[:, :min_dim]
            B_trim = B[:, :min_dim]

            # å½’ä¸€åŒ–
            A_norm = F.normalize(A_trim, p=2, dim=1)
            B_norm = F.normalize(B_trim, p=2, dim=1)

            # è®¡ç®—æ‰¹æ¬¡å†…ä½™å¼¦ç›¸ä¼¼åº¦çš„å¹³å‡å€¼
            cosine_sim = (A_norm * B_norm).sum(dim=1)  # [batch_size]

            # æˆ‘ä»¬å¸Œæœ›ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¿‘0ï¼ˆæ­£äº¤ï¼‰
            return cosine_sim.abs().mean()  # å–ç»å¯¹å€¼åå¹³å‡

        loss1 = dimension_aware_loss(eeg_common, eeg_private)
        loss2 = dimension_aware_loss(image_common, image_private)
        loss3 = dimension_aware_loss(eeg_private, image_private)

        total_loss = (loss1 + loss2 + loss3) / 3.0

        return total_loss

    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, mean=0.0, std=0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.1)


# æµ‹è¯•ä¿®æ”¹åçš„ç½‘ç»œ
if __name__ == "__main__":
    print("=== æµ‹è¯•å¤šæ¨¡æ€ç½‘ç»œ ===")


    model = MultiModalFusionNetwork(
        use_pretrained_eeg=True,  # æµ‹è¯•æ—¶è®¾ä¸ºFalseé¿å…åŠ è½½å®é™…æ–‡ä»¶
        use_pretrained_image=True,
    )
    print(model)

    # æµ‹è¯•è¾“å…¥
    eeg1 = torch.randn(2, 64, 2000)
    eeg2 = torch.randn(2, 64, 2000)
    img1 = torch.randn(2, 3, 224, 224)
    img2 = torch.randn(2, 3, 224, 224)

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        logits, eeg_common, image_common, eeg_private, image_private = model(eeg1, eeg2, img1, img2)

    print(f"è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")