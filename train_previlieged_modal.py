import torch
import torch.nn as nn
import torch.optim as optim
import os
import time
import warnings
import json
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

warnings.filterwarnings('ignore')
sns.set_theme()

from models.privileged_model import PrivilegedMultimodalNetwork
from MyPP2Dataset import MyPP2Dataset, create_dataloaders
from config.config_privileged_model import get_config


class PrivilegedTrainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆ›å»ºåŠ¨æ€è¾“å‡ºç›®å½•
        self.output_dir = config.get_output_dir()
        self.config.output_dir = self.output_dir  # æ›´æ–°configä¸­çš„è¾“å‡ºç›®å½•
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

        # åˆ›å»ºæ¨¡å‹
        self.model = self._create_model()

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()

        # è®­ç»ƒè®°å½•
        self.train_losses = []
        self.val_losses = []
        self.train_teacher_losses = []
        self.train_student_losses = []
        self.train_distill_losses = []
        self.train_feature_align_losses = []
        self.train_common_sim_losses = []
        self.train_private_diff_losses = []
        self.val_teacher_losses = []
        self.val_student_losses = []
        self.val_distill_losses = []
        self.val_feature_align_losses = []
        self.val_common_sim_losses = []
        self.val_private_diff_losses = []
        self.teacher_accuracies = []
        self.student_accuracies = []
        self.learning_rates = []

        self.best_val_loss = float('inf')
        self.best_teacher_acc = 0.0
        self.best_student_acc = 0.0
        self.current_epoch = 0

    def _create_model(self):
        """åˆ›å»ºç‰¹æƒå­¦ä¹ æ¨¡å‹"""
        model = PrivilegedMultimodalNetwork(
            eeg_model_name=self.config.eeg_model_name,
            image_model_name=self.config.image_model_name,
            image_model_type=self.config.image_model_type,
            in_chans=self.config.in_chans,
            n_classes=self.config.n_classes,
            input_window_samples=self.config.input_window_samples,
            use_pretrained_eeg=self.config.use_pretrained,
            use_pretrained_image=self.config.use_pretrained,
            base_path=self.config.base_path,
            common_dim=self.config.common_dim,
            private_dim=self.config.private_dim,
            dropout_rate=self.config.dropout_rate,
            alpha=self.config.alpha,
            beta=self.config.beta,
            gamma=self.config.gamma,
            temperature=self.config.temperature
        )

        return model.to(self.device)

    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if self.config.optimizer.lower() == 'adam':
            return optim.Adam(self.model.parameters(), lr=self.config.lr, weight_decay=self.config.weight_decay)
        elif self.config.optimizer.lower() == 'adamw':
            return optim.AdamW(self.model.parameters(), lr=self.config.lr, weight_decay=self.config.weight_decay)
        elif self.config.optimizer.lower() == 'sgd':
            return optim.SGD(self.model.parameters(), lr=self.config.lr, momentum=self.config.momentum,
                             weight_decay=self.config.weight_decay)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.config.optimizer}")

    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.config.scheduler.lower() == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.epochs
            )
        elif self.config.scheduler.lower() == 'step':
            return optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=30,
                gamma=0.1
            )
        elif self.config.scheduler.lower() == 'plateau':
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                patience=self.config.patience,
                factor=self.config.factor,
                verbose=True
            )
        else:
            return None

    def create_dataloaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")

        # åˆ›å»ºæ•°æ®é›†
        dataset = MyPP2Dataset(
            is_flipped=False,
            transform=self.config.transform,
            subject_id=self.config.subject_id
        )
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_dataloaders(
            dataset=dataset,
            batch_size=self.config.batch_size,
            shuffle=True
        )

        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        train_labels = [label for _, _, _, _, label in train_loader.dataset]
        val_labels = [label for _, _, _, _, label in val_loader.dataset]

        print(f"è®­ç»ƒé›†åˆ†å¸ƒ: {torch.bincount(torch.tensor(train_labels)).tolist()}")
        print(f"éªŒè¯é›†åˆ†å¸ƒ: {torch.bincount(torch.tensor(val_labels)).tolist()}")
        print(f"è®­ç»ƒé›†: {len(train_loader.dataset)}, éªŒè¯é›†: {len(val_loader.dataset)}")

        return train_loader, val_loader

    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_teacher_loss = 0
        total_student_loss = 0
        total_distill_loss = 0
        total_feature_align_loss = 0
        total_common_sim_loss = 0
        total_private_diff_loss = 0

        for batch_idx, (left_img, right_img, left_eeg, right_eeg, labels) in enumerate(train_loader):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            left_img = left_img.to(self.device)
            right_img = right_img.to(self.device)
            left_eeg = left_eeg.to(self.device)
            right_eeg = right_eeg.to(self.device)
            labels = labels.to(self.device)

            # æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = self.model(
                eeg1=left_eeg, eeg2=right_eeg,
                img1=left_img, img2=right_img,
                mode='train'
            )

            # è®¡ç®—æŸå¤±
            loss_dict = self.model.compute_loss(outputs, labels)
            loss = loss_dict['total_loss']

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            if self.config.grad_clip > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config.grad_clip
                )

            # æ›´æ–°å‚æ•°
            self.optimizer.step()

            # è®°å½•æ‰€æœ‰æŸå¤±
            total_loss += loss.item()
            total_teacher_loss += loss_dict['teacher_loss'].item()
            total_student_loss += loss_dict['student_loss'].item()
            total_distill_loss += loss_dict['distill_loss'].item()
            total_feature_align_loss += loss_dict['feature_align_loss'].item()
            total_common_sim_loss += loss_dict['common_sim_loss'].item()
            total_private_diff_loss += loss_dict['private_diff_loss'].item()

            # # æ‰“å°è¿›åº¦ - æ‰“å°æ‰€æœ‰7ä¸ªæŸå¤±
            # if batch_idx % self.config.log_interval == 0:
            #     print(f'è®­ç»ƒæ‰¹æ¬¡ [{batch_idx}/{len(train_loader)}]')
            #     print(f'  æ€»æŸå¤±: {loss.item():.6f}')
            #     print(f'  æ•™å¸ˆæŸå¤±: {loss_dict["teacher_loss"].item():.6f}')
            #     print(f'  å­¦ç”ŸæŸå¤±: {loss_dict["student_loss"].item():.6f}')
            #     print(f'  è’¸é¦æŸå¤±: {loss_dict["distill_loss"].item():.6f}')
            #     print(f'  ç‰¹å¾å¯¹é½æŸå¤±: {loss_dict["feature_align_loss"].item():.6f}')
            #     print(f'  å…¬å…±ç‰¹å¾ç›¸ä¼¼æŸå¤±: {loss_dict["common_sim_loss"].item():.6f}')
            #     print(f'  ç§æœ‰ç‰¹å¾å·®å¼‚æŸå¤±: {loss_dict["private_diff_loss"].item():.6f}')
            #     print('  ' + '-' * 40)

        # è®¡ç®—å¹³å‡æŸå¤±
        num_batches = len(train_loader)
        return {
            'total_loss': total_loss / num_batches,
            'teacher_loss': total_teacher_loss / num_batches,
            'student_loss': total_student_loss / num_batches,
            'distill_loss': total_distill_loss / num_batches,
            'feature_align_loss': total_feature_align_loss / num_batches,
            'common_sim_loss': total_common_sim_loss / num_batches,
            'private_diff_loss': total_private_diff_loss / num_batches
        }

    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0
        total_teacher_loss = 0
        total_student_loss = 0
        total_distill_loss = 0
        total_feature_align_loss = 0
        total_common_sim_loss = 0
        total_private_diff_loss = 0

        # è®¡ç®—å‡†ç¡®ç‡
        teacher_correct = 0
        student_correct = 0
        total_samples = 0

        with torch.no_grad():
            for batch_idx, (left_img, right_img, left_eeg, right_eeg, labels) in enumerate(val_loader):
                # ç§»åŠ¨åˆ°è®¾å¤‡
                left_img = left_img.to(self.device)
                right_img = right_img.to(self.device)
                left_eeg = left_eeg.to(self.device)
                right_eeg = right_eeg.to(self.device)
                labels = labels.to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    eeg1=left_eeg, eeg2=right_eeg,
                    img1=left_img, img2=right_img,
                    mode='train'
                )

                # è®¡ç®—æŸå¤±
                loss_dict = self.model.compute_loss(outputs, labels)
                loss = loss_dict['total_loss']

                # è®°å½•æ‰€æœ‰æŸå¤±
                total_loss += loss.item()
                total_teacher_loss += loss_dict['teacher_loss'].item()
                total_student_loss += loss_dict['student_loss'].item()
                total_distill_loss += loss_dict['distill_loss'].item()
                total_feature_align_loss += loss_dict['feature_align_loss'].item()
                total_common_sim_loss += loss_dict['common_sim_loss'].item()
                total_private_diff_loss += loss_dict['private_diff_loss'].item()

                # è®¡ç®—å‡†ç¡®ç‡
                teacher_logits = outputs['teacher_logits']
                student_logits = outputs['student_logits']

                if teacher_logits.dim() == 1:  # äºŒåˆ†ç±»
                    teacher_preds = (torch.sigmoid(teacher_logits) > 0.5).float()
                    student_preds = (torch.sigmoid(student_logits) > 0.5).float()
                else:  # å¤šåˆ†ç±»
                    teacher_preds = torch.argmax(teacher_logits, dim=1)
                    student_preds = torch.argmax(student_logits, dim=1)

                teacher_correct += (teacher_preds == labels).sum().item()
                student_correct += (student_preds == labels).sum().item()
                total_samples += labels.size(0)

                # # æ‰“å°éªŒè¯è¿›åº¦
                # if batch_idx % self.config.log_interval == 0:
                #     print(f'éªŒè¯æ‰¹æ¬¡ [{batch_idx}/{len(val_loader)}]')
                #     print(f'  æ€»æŸå¤±: {loss.item():.6f}')
                #     print(f'  æ•™å¸ˆæŸå¤±: {loss_dict["teacher_loss"].item():.6f}')
                #     print(f'  å­¦ç”ŸæŸå¤±: {loss_dict["student_loss"].item():.6f}')
                #     print(f'  è’¸é¦æŸå¤±: {loss_dict["distill_loss"].item():.6f}')
                #     print(f'  ç‰¹å¾å¯¹é½æŸå¤±: {loss_dict["feature_align_loss"].item():.6f}')
                #     print(f'  å…¬å…±ç‰¹å¾ç›¸ä¼¼æŸå¤±: {loss_dict["common_sim_loss"].item():.6f}')
                #     print(f'  ç§æœ‰ç‰¹å¾å·®å¼‚æŸå¤±: {loss_dict["private_diff_loss"].item():.6f}')
                #     print('  ' + '-' * 40)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        num_batches = len(val_loader)
        teacher_acc = teacher_correct / total_samples * 100
        student_acc = student_correct / total_samples * 100

        return {
            'total_loss': total_loss / num_batches,
            'teacher_loss': total_teacher_loss / num_batches,
            'student_loss': total_student_loss / num_batches,
            'distill_loss': total_distill_loss / num_batches,
            'feature_align_loss': total_feature_align_loss / num_batches,
            'common_sim_loss': total_common_sim_loss / num_batches,
            'private_diff_loss': total_private_diff_loss / num_batches,
            'teacher_acc': teacher_acc,
            'student_acc': student_acc
        }

    def _save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²åˆ°JSONæ–‡ä»¶"""
        history = {
            'epochs': list(range(1, self.current_epoch + 1)),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_teacher_losses': self.train_teacher_losses,
            'train_student_losses': self.train_student_losses,
            'train_distill_losses': self.train_distill_losses,
            'val_teacher_losses': self.val_teacher_losses,
            'val_student_losses': self.val_student_losses,
            'val_distill_losses': self.val_distill_losses,
            'teacher_accuracies': self.teacher_accuracies,
            'student_accuracies': self.student_accuracies,
            'learning_rates': self.learning_rates,
            'best_val_loss': self.best_val_loss,
            'best_teacher_acc': self.best_teacher_acc,
            'best_student_acc': self.best_student_acc,
            'end_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        history_path = os.path.join(self.output_dir, "training_history.json")
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

        print("è®­ç»ƒå†å²å·²ä¿å­˜")

    def _plot_training_results(self):
        """ç»˜åˆ¶è®­ç»ƒç»“æœå›¾è¡¨"""
        epochs = range(1, len(self.train_losses) + 1)

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Privileged Learning Training Results', fontsize=16)

        # 1. æ€»æŸå¤±æ›²çº¿
        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='Train Loss', alpha=0.7)
        axes[0, 0].plot(epochs, self.val_losses, 'r-', label='Val Loss', alpha=0.7)
        axes[0, 0].set_title('Total Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. è¯¦ç»†æŸå¤±æ›²çº¿
        axes[0, 1].plot(epochs, self.train_teacher_losses, 'b-', label='Train Teacher', alpha=0.7)
        axes[0, 1].plot(epochs, self.train_student_losses, 'g-', label='Train Student', alpha=0.7)
        axes[0, 1].plot(epochs, self.train_distill_losses, 'orange', label='Train Distill', alpha=0.7)
        axes[0, 1].set_title('Training Loss Components')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. å‡†ç¡®ç‡æ›²çº¿
        axes[1, 0].plot(epochs, self.teacher_accuracies, 'b-', label='Teacher Acc', alpha=0.7)
        axes[1, 0].plot(epochs, self.student_accuracies, 'r-', label='Student Acc', alpha=0.7)
        axes[1, 0].set_title('Validation Accuracy')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Accuracy (%)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # 4. å­¦ä¹ ç‡æ›²çº¿
        axes[1, 1].plot(epochs, self.learning_rates, 'purple', alpha=0.7)
        axes[1, 1].set_title('Learning Rate')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].set_yscale('log')

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(self.output_dir, "training_plots.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"è®­ç»ƒå›¾è¡¨å·²ä¿å­˜: {plot_path}")

    def train(self):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("å¼€å§‹è®­ç»ƒç‰¹æƒå­¦ä¹ ç½‘ç»œ...")
        start_time = time.time()

        # æ¢å¤è®­ç»ƒ
        if self.config.resume_training and self.config.checkpoint_path:
            self.load_checkpoint(self.config.checkpoint_path)
            print(f"ä» epoch {self.current_epoch} æ¢å¤è®­ç»ƒ")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = self.create_dataloaders()

        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.current_epoch + 1, self.config.epochs + 1):
            self.current_epoch = epoch
            print(f"\nEpoch {epoch}/{self.config.epochs}")
            print("-" * 50)

            # è®­ç»ƒ
            epoch_start = time.time()
            train_metrics = self.train_epoch(train_loader)
            epoch_time = time.time() - epoch_start

            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)

            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            self.learning_rates.append(current_lr)

            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics['total_loss'])
                else:
                    self.scheduler.step()

            # è®°å½•æ‰€æœ‰æŸå¤±å’ŒæŒ‡æ ‡
            self.train_losses.append(train_metrics['total_loss'])
            self.val_losses.append(val_metrics['total_loss'])
            self.train_teacher_losses.append(train_metrics['teacher_loss'])
            self.train_student_losses.append(train_metrics['student_loss'])
            self.train_distill_losses.append(train_metrics['distill_loss'])
            self.train_feature_align_losses.append(train_metrics['feature_align_loss'])
            self.train_common_sim_losses.append(train_metrics['common_sim_loss'])
            self.train_private_diff_losses.append(train_metrics['private_diff_loss'])
            self.val_teacher_losses.append(val_metrics['teacher_loss'])
            self.val_student_losses.append(val_metrics['student_loss'])
            self.val_distill_losses.append(val_metrics['distill_loss'])
            self.val_feature_align_losses.append(val_metrics['feature_align_loss'])
            self.val_common_sim_losses.append(val_metrics['common_sim_loss'])
            self.val_private_diff_losses.append(val_metrics['private_diff_loss'])
            self.teacher_accuracies.append(val_metrics['teacher_acc'])
            self.student_accuracies.append(val_metrics['student_acc'])

            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            if val_metrics['total_loss'] < self.best_val_loss:
                self.best_val_loss = val_metrics['total_loss']
            if val_metrics['teacher_acc'] > self.best_teacher_acc:
                self.best_teacher_acc = val_metrics['teacher_acc']
            if val_metrics['student_acc'] > self.best_student_acc:
                self.best_student_acc = val_metrics['student_acc']

            # æ‰“å°æ‰€æœ‰æŒ‡æ ‡ - è¯¦ç»†æ ¼å¼
            print(f"\nğŸ“Š Epoch {epoch} è®­ç»ƒç»“æœ:")
            print(f"  å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"  ğŸ¯ è®­ç»ƒæŸå¤±:")
            print(f"    â€¢ æ€»æŸå¤±: {train_metrics['total_loss']:.6f}")
            print(f"    â€¢ æ•™å¸ˆæŸå¤±: {train_metrics['teacher_loss']:.6f}")
            print(f"    â€¢ å­¦ç”ŸæŸå¤±: {train_metrics['student_loss']:.6f}")
            print(f"    â€¢ è’¸é¦æŸå¤±: {train_metrics['distill_loss']:.6f}")
            print(f"    â€¢ ç‰¹å¾å¯¹é½æŸå¤±: {train_metrics['feature_align_loss']:.6f}")
            print(f"    â€¢ å…¬å…±ç‰¹å¾ç›¸ä¼¼æŸå¤±: {train_metrics['common_sim_loss']:.6f}")
            print(f"    â€¢ ç§æœ‰ç‰¹å¾å·®å¼‚æŸå¤±: {train_metrics['private_diff_loss']:.6f}")
            print(f"  ğŸ¯ éªŒè¯æŸå¤±:")
            print(f"    â€¢ æ€»æŸå¤±: {val_metrics['total_loss']:.6f}")
            print(f"    â€¢ æ•™å¸ˆæŸå¤±: {val_metrics['teacher_loss']:.6f}")
            print(f"    â€¢ å­¦ç”ŸæŸå¤±: {val_metrics['student_loss']:.6f}")
            print(f"    â€¢ è’¸é¦æŸå¤±: {val_metrics['distill_loss']:.6f}")
            print(f"    â€¢ ç‰¹å¾å¯¹é½æŸå¤±: {val_metrics['feature_align_loss']:.6f}")
            print(f"    â€¢ å…¬å…±ç‰¹å¾ç›¸ä¼¼æŸå¤±: {val_metrics['common_sim_loss']:.6f}")
            print(f"    â€¢ ç§æœ‰ç‰¹å¾å·®å¼‚æŸå¤±: {val_metrics['private_diff_loss']:.6f}")
            print(f"  ğŸ“ˆ å‡†ç¡®ç‡:")
            print(f"    â€¢ æ•™å¸ˆ: {val_metrics['teacher_acc']:.2f}%")
            print(f"    â€¢ å­¦ç”Ÿ: {val_metrics['student_acc']:.2f}%")
            print(f"  â±ï¸  æ—¶é—´: {epoch_time:.2f}s")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['total_loss'] < self.best_val_loss:
                self.save_checkpoint(is_best=True)
                print("âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.config.save_interval == 0:
                self.save_checkpoint(is_best=False)
                print("ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹")

            # å®šæœŸä¿å­˜è®­ç»ƒå†å²å’Œå›¾è¡¨
            if epoch % self.config.save_interval == 0 or epoch == self.config.epochs:
                self._save_training_history()
                if epoch > 1:  # è‡³å°‘æœ‰ä¸¤ä¸ªepochæ‰èƒ½ç”»å›¾
                    self._plot_training_results()

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"\nè®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time:.2f}s")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        print(f"æœ€ä½³æ•™å¸ˆå‡†ç¡®ç‡: {self.best_teacher_acc:.2f}%")
        print(f"æœ€ä½³å­¦ç”Ÿå‡†ç¡®ç‡: {self.best_student_acc:.2f}%")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œç»“æœ
        self.save_checkpoint(is_best=False, is_final=True)
        self._save_training_history()
        self._plot_training_results()

        return self.train_losses, self.val_losses

    def save_checkpoint(self, is_best=False, is_final=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_teacher_losses': self.train_teacher_losses,
            'train_student_losses': self.train_student_losses,
            'train_distill_losses': self.train_distill_losses,
            'val_teacher_losses': self.val_teacher_losses,
            'val_student_losses': self.val_student_losses,
            'val_distill_losses': self.val_distill_losses,
            'teacher_accuracies': self.teacher_accuracies,
            'student_accuracies': self.student_accuracies,
            'learning_rates': self.learning_rates,
            'best_val_loss': self.best_val_loss,
            'best_teacher_acc': self.best_teacher_acc,
            'best_student_acc': self.best_student_acc,
            'config': self.config.to_dict()
        }

        if is_best:
            filename = "privileged_best_model.pth"
        elif is_final:
            filename = f"privileged_final_model_epoch{self.current_epoch}.pth"
        else:
            filename = f"checkpoints/privileged_checkpoint_epoch{self.current_epoch}.pth"

        filepath = os.path.join(self.output_dir, filename)
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        torch.save(checkpoint, filepath)

        # åŒæ—¶ä¿å­˜å­¦ç”Ÿç½‘ç»œçš„ç‹¬ç«‹æ¨¡å‹ï¼ˆç”¨äºæ¨ç†ï¼‰
        if is_best or is_final:
            student_model = self.model.module.student_network if hasattr(self.model,
                                                                         'module') else self.model.student_network
            student_checkpoint = {
                'student_state_dict': student_model.state_dict(),
                'config': self.config.to_dict()
            }
            student_filepath = os.path.join(self.output_dir, f"student_{filename}")
            torch.save(student_checkpoint, student_filepath)

    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if self.scheduler and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        # æ¢å¤è®­ç»ƒå†å²
        self.train_losses = checkpoint.get('train_losses', [])
        self.val_losses = checkpoint.get('val_losses', [])
        self.train_teacher_losses = checkpoint.get('train_teacher_losses', [])
        self.train_student_losses = checkpoint.get('train_student_losses', [])
        self.train_distill_losses = checkpoint.get('train_distill_losses', [])
        self.val_teacher_losses = checkpoint.get('val_teacher_losses', [])
        self.val_student_losses = checkpoint.get('val_student_losses', [])
        self.val_distill_losses = checkpoint.get('val_distill_losses', [])
        self.teacher_accuracies = checkpoint.get('teacher_accuracies', [])
        self.student_accuracies = checkpoint.get('student_accuracies', [])
        self.learning_rates = checkpoint.get('learning_rates', [])

        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        self.best_teacher_acc = checkpoint.get('best_teacher_acc', 0.0)
        self.best_student_acc = checkpoint.get('best_student_acc', 0.0)
        self.current_epoch = checkpoint['epoch']

        print(f"åŠ è½½æ£€æŸ¥ç‚¹: epoch {self.current_epoch}, æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")

        return self.current_epoch


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è·å–é…ç½®
    config = get_config('default')  # å¯é€‰: 'default', 'debug', 'large', 'small'

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PrivilegedTrainer(config)

    # å¼€å§‹è®­ç»ƒ
    train_losses, val_losses = trainer.train()

    print("\nè®­ç»ƒå®Œæˆ!")
    print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
    print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {trainer.output_dir}")


if __name__ == "__main__":
    main()