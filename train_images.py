import os
import torch
import torch.nn as nn
import torch.optim as optim
from matplotlib import pyplot as plt
import json
import pandas as pd
from datetime import datetime

import config.config_image_model
from MyPP2Dataset import MyPP2Dataset, create_dataloaders, create_subject_dataloaders
from models.image_models import SSCNN, RSSCNN
from utils.save_confusion_matrix import save_confusion_matrix
from tqdm import tqdm

from utils.early_stop import SchedulerEarlyStopper


class ImageModelTrainer:
    def __init__(self, model_type, base_model_name, timestamp):
        """
        å›¾åƒæ¨¡å‹è®­ç»ƒå™¨åŸºç±»

        Args:
            model_type: æ¨¡å‹ç±»å‹ ('sscnn' æˆ– 'rsscnn')
            base_model_name: åŸºç¡€æ¨¡å‹åç§°
            timestamp: æ—¶é—´æˆ³
        """
        self.model_type = model_type
        self.base_model_name = base_model_name
        self.timestamp = timestamp

        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.setup_directories()

    def setup_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        # åŸºç¡€ç›®å½•
        self.base_dir = f"outputs/outputs_images/{self.model_type}/{self.base_model_name}/{self.timestamp}"

        # å­ç›®å½•
        self.model_dir = os.path.join(self.base_dir, "models")
        self.plot_dir = os.path.join(self.base_dir, "plots")
        self.confusion_matrix_dir = os.path.join(self.base_dir, "confusion_matrices")
        self.data_dir = os.path.join(self.base_dir, "training_data")
        self.checkpoints_dir = os.path.join(self.base_dir, "checkpoints")  # æ–°å¢æ£€æŸ¥ç‚¹ç›®å½•

        # åˆ›å»ºæ‰€æœ‰ç›®å½•
        for directory in [self.model_dir, self.plot_dir, self.confusion_matrix_dir,
                         self.data_dir, self.checkpoints_dir]:
            os.makedirs(directory, exist_ok=True)

        print(f"è¾“å‡ºç›®å½•ç»“æ„:")
        print(f"  - åŸºç¡€ç›®å½•: {self.base_dir}")
        print(f"  - æ¨¡å‹ç›®å½•: {self.model_dir}")
        print(f"  - æ£€æŸ¥ç‚¹ç›®å½•: {self.checkpoints_dir}")
        print(f"  - å›¾åƒç›®å½•: {self.plot_dir}")
        print(f"  - æ··æ·†çŸ©é˜µ: {self.confusion_matrix_dir}")
        print(f"  - è®­ç»ƒæ•°æ®: {self.data_dir}")


# è®­ç»ƒSSCNNæ¨¡å‹
def train_sscnn(model_name='AlexNet', num_epochs=300, lr=0.001, batch_size=4, early_stopper=None,
                momentum=0.9,
                weight_decay=1e-4,
                factor=0.1,
                patience=10,
                device=None,
                my_dataset=None):
    # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    trainer = ImageModelTrainer("sscnn", model_name, timestamp)

    train_loader, val_loader = create_dataloaders(my_dataset, batch_size=batch_size, shuffle=True)

    # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
    train_labels = [label for _, _, _, _, label in train_loader.dataset]
    test_labels = [label for _, _, _, _, label in val_loader.dataset]
    print("==========================æ£€æŸ¥æ•°æ®é›†åˆ†å¸ƒ==========================")
    print(f"è®­ç»ƒé›†åˆ†å¸ƒ: {torch.bincount(torch.tensor(train_labels)).tolist()}")
    print(f"æµ‹è¯•é›†åˆ†å¸ƒ: {torch.bincount(torch.tensor(test_labels)).tolist()}")
    print(f"è®­ç»ƒé›†: {len(train_loader.dataset)}, æµ‹è¯•é›†: {len(val_loader.dataset)}")

    model = SSCNN(base_model_name=model_name).to(device)
    model.device = device
    criterion = nn.CrossEntropyLoss()
    # AlexNet PlacesNet
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)
    # VGG
    # optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=factor, patience=patience)

    # æ‰©å±•å†å²è®°å½•ä»¥åŒ…å«æ›´å¤šä¿¡æ¯
    history = {
        'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],
        'learning_rates': [], 'epoch_times': [], 'best_epoch': 0,
        'checkpoint_epochs': []  # è®°å½•ä¿å­˜æ£€æŸ¥ç‚¹çš„epoch
    }

    # è®­ç»ƒé…ç½®ä¿¡æ¯
    train_config = {
        'model_type': 'SSCNN',
        'base_model_name': model_name,
        'num_epochs': num_epochs,
        'learning_rate': lr,
        'batch_size': batch_size,
        'momentum': momentum,
        'factor': factor,
        'patience': patience,
        'timestamp': timestamp,
        'device': str(device)
    }

    best_acc = 0.0
    current_lr = lr

    for epoch in range(num_epochs):
        epoch_start_time = datetime.now()

        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for left_imgs, right_imgs, _, _, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):
            left_imgs = left_imgs.to(device)
            right_imgs = right_imgs.to(device)
            labels = labels.to(device)

            outputs = model(left_imgs, right_imgs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_loss = running_loss / len(train_loader)
        train_acc = 100 * correct / total

        val_loss, val_acc, val_labels, val_preds = evaluate_sscnn(model, val_loader, criterion, return_preds=True)

        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['learning_rates'].append(current_lr)
        history['epoch_times'].append((datetime.now() - epoch_start_time).total_seconds())

        # å­¦ä¹ ç‡æ›´æ–°
        old_lr = current_lr
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        if current_lr != old_lr:
            print(f"å­¦ä¹ ç‡æ›´æ–°: {old_lr:.6f} -> {current_lr:.6f}")

        # ä¿å­˜æ··æ·†çŸ©é˜µ
        save_confusion_matrix(val_labels, val_preds,
                              class_names=['Left', 'Right'],
                              title=f"SSCNN {model_name} - Epoch {epoch + 1}",
                              filename=os.path.join(trainer.confusion_matrix_dir, f"confmat_epoch{epoch + 1}.png")
                              )

        # 1. ä¿å­˜éªŒè¯é›†æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            history['best_epoch'] = epoch + 1

            model_path = os.path.join(trainer.model_dir, f'best_val_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss,
                'train_acc': train_acc,
                'train_loss': train_loss,
                'config': train_config,
                'model_type': 'best_val'  # æ ‡è®°ä¸ºæœ€ä½³éªŒè¯æ¨¡å‹
            }, model_path)

            # ä¿å­˜æœ€ä½³æ··æ·†çŸ©é˜µ
            save_confusion_matrix(val_labels, val_preds,
                                  class_names=['Left', 'Right'],
                                  title=f"SSCNN {model_name} - Best Val Epoch {epoch + 1}",
                                  filename=os.path.join(trainer.confusion_matrix_dir, "confmat_best_val.png")
                                  )
            print(f"âœ… ä¿å­˜æœ€ä½³éªŒè¯æ¨¡å‹ - å‡†ç¡®ç‡: {val_acc:.2f}%")

        # 2. æ¯10ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 10 == 0 or epoch == 0:  # ç¬¬ä¸€ä¸ªepochä¹Ÿä¿å­˜
            checkpoint_path = os.path.join(trainer.checkpoints_dir, f'checkpoint_epoch_{epoch + 1}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'train_acc': train_acc,
                'val_acc': val_acc,
                'learning_rate': current_lr,
                'config': train_config,
                'model_type': 'checkpoint'
            }, checkpoint_path)
            history['checkpoint_epochs'].append(epoch + 1)
            print(f"ğŸ“ ä¿å­˜æ£€æŸ¥ç‚¹ - Epoch {epoch + 1}")

        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

        # æ—©åœåˆ¤æ–­
        if early_stopper.step(optimizer):
            print("è®­ç»ƒæå‰ç»ˆæ­¢ï¼šå­¦ä¹ ç‡å·²ç»è¡°å‡è¾¾åˆ°æœ€å¤§æ¬¡æ•°ã€‚")
            break

    # 3. è®­ç»ƒå®Œæˆåä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(trainer.model_dir, "final_model.pth")
    torch.save({
        'epoch': num_epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'train_acc': train_acc,
        'val_acc': val_acc,
        'config': train_config,
        'history': history,
        'model_type': 'final'
    }, final_model_path)
    print(f"ğŸ ä¿å­˜æœ€ç»ˆæ¨¡å‹ - Epoch {num_epochs}")

    # ä¿å­˜æ‰€æœ‰è®­ç»ƒç»“æœ
    save_training_results(history, train_config, trainer, model, "SSCNN")

    return model, history, trainer.base_dir


# è®­ç»ƒRSSCNNæ¨¡å‹
def train_rsscnn(model_name='AlexNet', num_epochs=300, lr=0.001, lambda_r=0.1, batch_size=4,
                 early_stopper=None,
                 momentum=0.9,
                 weight_decay=1e-4,
                 factor=0.1,
                 patience=10,
                 device=None, my_dataset_1=None):
    # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    trainer = ImageModelTrainer("rsscnn", model_name, timestamp)

    train_loader, val_loader = create_dataloaders(my_dataset_1, batch_size=batch_size, shuffle=True)

    # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
    train_labels = [label for _, _, _, _, label in train_loader.dataset]
    test_labels = [label for _, _, _, _, label in val_loader.dataset]
    print("==========================æ£€æŸ¥æ•°æ®é›†åˆ†å¸ƒ==========================")
    print(f"è®­ç»ƒé›†åˆ†å¸ƒ: {torch.bincount(torch.tensor(train_labels)).tolist()}")
    print(f"æµ‹è¯•é›†åˆ†å¸ƒ: {torch.bincount(torch.tensor(test_labels)).tolist()}")
    print(f"è®­ç»ƒé›†: {len(train_loader.dataset)}, æµ‹è¯•é›†: {len(val_loader.dataset)}")

    model = RSSCNN(base_model_name=model_name, lambda_r=lambda_r).to(device)
    model.device = device
    # AlexNet placesNet
    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=factor, patience=patience)

    # æ‰©å±•å†å²è®°å½•
    history = {
        'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],
        'learning_rates': [], 'epoch_times': [], 'best_epoch': 0,
        'checkpoint_epochs': []
    }

    # è®­ç»ƒé…ç½®ä¿¡æ¯
    train_config = {
        'model_type': 'RSSCNN',
        'base_model_name': model_name,
        'num_epochs': num_epochs,
        'learning_rate': lr,
        'lambda_r': lambda_r,
        'batch_size': batch_size,
        'momentum': momentum,
        'factor': factor,
        'patience': patience,
        'timestamp': timestamp,
        'device': str(device)
    }

    best_acc = 0.0
    current_lr = lr

    for epoch in range(num_epochs):
        epoch_start_time = datetime.now()

        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for left_imgs, right_imgs, _, _, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):
            left_imgs, right_imgs, labels = left_imgs.to(device), right_imgs.to(device), labels.to(device)

            class_out, rank1, rank2 = model(left_imgs, right_imgs)
            loss = model.compute_loss(class_out, rank1, rank2, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(class_out.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_loss = running_loss / len(train_loader)
        train_acc = 100 * correct / total

        val_loss, val_acc, val_labels, val_preds = evaluate_rsscnn(model, val_loader, return_preds=True)

        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['learning_rates'].append(current_lr)
        history['epoch_times'].append((datetime.now() - epoch_start_time).total_seconds())

        # å­¦ä¹ ç‡æ›´æ–°
        old_lr = current_lr
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        if current_lr != old_lr:
            print(f"å­¦ä¹ ç‡æ›´æ–°: {old_lr:.6f} -> {current_lr:.6f}")

        # ä¿å­˜æ··æ·†çŸ©é˜µ
        save_confusion_matrix(val_labels, val_preds,
                              class_names=['Left', 'Right'],
                              title=f"RSSCNN {model_name} - Epoch {epoch + 1}",
                              filename=os.path.join(trainer.confusion_matrix_dir, f"confmat_epoch{epoch + 1}.png")
                              )

        # 1. ä¿å­˜éªŒè¯é›†æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            history['best_epoch'] = epoch + 1

            model_path = os.path.join(trainer.model_dir, f'best_val_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss,
                'train_acc': train_acc,
                'train_loss': train_loss,
                'config': train_config,
                'model_type': 'best_val'
            }, model_path)

            # ä¿å­˜æœ€ä½³æ··æ·†çŸ©é˜µ
            save_confusion_matrix(val_labels, val_preds,
                                  class_names=['Left', 'Right'],
                                  title=f"RSSCNN {model_name} - Best Val Epoch {epoch + 1}",
                                  filename=os.path.join(trainer.confusion_matrix_dir, "confmat_best_val.png")
                                  )
            print(f"âœ… ä¿å­˜æœ€ä½³éªŒè¯æ¨¡å‹ - å‡†ç¡®ç‡: {val_acc:.2f}%")

        # 2. æ¯10ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 10 == 0 or epoch == 0:  # ç¬¬ä¸€ä¸ªepochä¹Ÿä¿å­˜
            checkpoint_path = os.path.join(trainer.checkpoints_dir, f'checkpoint_epoch_{epoch + 1}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'train_acc': train_acc,
                'val_acc': val_acc,
                'learning_rate': current_lr,
                'config': train_config,
                'model_type': 'checkpoint'
            }, checkpoint_path)
            history['checkpoint_epochs'].append(epoch + 1)
            print(f"ğŸ“ ä¿å­˜æ£€æŸ¥ç‚¹ - Epoch {epoch + 1}")

        print(f"Epoch [{epoch + 1}/{num_epochs}], "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")

        # æ—©åœåˆ¤æ–­
        if early_stopper.step(optimizer):
            print("è®­ç»ƒæå‰ç»ˆæ­¢ï¼šå­¦ä¹ ç‡å·²ç»è¡°å‡è¾¾åˆ°æœ€å¤§æ¬¡æ•°ã€‚")
            break

    # 3. è®­ç»ƒå®Œæˆåä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(trainer.model_dir, "final_model.pth")
    torch.save({
        'epoch': num_epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'train_acc': train_acc,
        'val_acc': val_acc,
        'config': train_config,
        'history': history,
        'model_type': 'final'
    }, final_model_path)
    print(f"ğŸ ä¿å­˜æœ€ç»ˆæ¨¡å‹ - Epoch {num_epochs}")

    # ä¿å­˜æ‰€æœ‰è®­ç»ƒç»“æœ
    save_training_results(history, train_config, trainer, model, "RSSCNN")

    return model, history, trainer.base_dir


def save_training_results(history, train_config, trainer, model, model_name):
    """ä¿å­˜è®­ç»ƒç»“æœï¼ŒåŒ…æ‹¬å›¾è¡¨å’Œæ•°æ®"""

    # 1. ä¿å­˜è®­ç»ƒé…ç½®
    config_path = os.path.join(trainer.data_dir, "training_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(train_config, f, indent=2, ensure_ascii=False)

    # 2. ä¿å­˜è®­ç»ƒå†å²æ•°æ®ä¸ºCSV
    df_data = {
        'epoch': list(range(1, len(history['train_loss']) + 1)),
        'train_loss': history['train_loss'],
        'train_acc': history['train_acc'],
        'val_loss': history['val_loss'],
        'val_acc': history['val_acc'],
        'learning_rate': history['learning_rates'],
        'epoch_time_seconds': history['epoch_times']
    }

    df = pd.DataFrame(df_data)
    csv_path = os.path.join(trainer.data_dir, "training_history.csv")
    df.to_csv(csv_path, index=False)

    # 3. ä¿å­˜è®­ç»ƒæ‘˜è¦
    summary = {
        'best_epoch': history['best_epoch'],
        'best_val_accuracy': max(history['val_acc']) if history['val_acc'] else 0,
        'best_train_accuracy': max(history['train_acc']) if history['train_acc'] else 0,
        'final_val_accuracy': history['val_acc'][-1] if history['val_acc'] else 0,
        'final_train_accuracy': history['train_acc'][-1] if history['train_acc'] else 0,
        'total_training_time_seconds': sum(history['epoch_times']),
        'total_epochs': len(history['train_loss']),
        'final_learning_rate': history['learning_rates'][-1] if history['learning_rates'] else 0,
        'checkpoint_epochs': history['checkpoint_epochs']
    }

    summary_path = os.path.join(trainer.data_dir, "training_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    # 4. ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæ›²çº¿
    plot_training_curves(history, model_name, train_config['base_model_name'], trainer.plot_dir)

    print(f"\nè®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {trainer.base_dir}")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {summary['best_val_accuracy']:.2f}% (ç¬¬ {summary['best_epoch']} è½®)")
    print(f"æ£€æŸ¥ç‚¹ä¿å­˜è½®æ¬¡: {summary['checkpoint_epochs']}")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {summary['total_training_time_seconds']:.2f} ç§’")


def plot_training_curves(history, model_name, base_model_name, plot_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    epochs = range(1, len(history['train_loss']) + 1)

    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)
    ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)
    ax1.set_title(f'{model_name} {base_model_name} - Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)
    ax2.plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)
    ax2.set_title(f'{model_name} {base_model_name} - Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # å­¦ä¹ ç‡æ›²çº¿
    ax3.plot(epochs, history['learning_rates'], 'g-', label='Learning Rate', linewidth=2)
    ax3.set_title(f'{model_name} {base_model_name} - Learning Rate')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Learning Rate')
    ax3.set_yscale('log')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # æ¯ä¸ªepochçš„è®­ç»ƒæ—¶é—´
    ax4.plot(epochs, history['epoch_times'], 'purple', label='Epoch Time', linewidth=2)
    ax4.set_title(f'{model_name} {base_model_name} - Epoch Training Time')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Time (seconds)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    plot_path = os.path.join(plot_dir, "training_curves.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")


# è¯„ä¼°SSCNNæ¨¡å‹
def evaluate_sscnn(model, dataloader, criterion, return_preds=False):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0

    all_labels = []
    all_preds = []

    with torch.no_grad():
        for left_imgs, right_imgs, _, _, labels in dataloader:
            left_imgs = left_imgs.to(model.device)
            right_imgs = right_imgs.to(model.device)
            labels = labels.to(model.device)

            outputs = model(left_imgs, right_imgs)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            if return_preds:
                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(predicted.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total

    if return_preds:
        return avg_loss, accuracy, all_labels, all_preds
    else:
        return avg_loss, accuracy


# è¯„ä¼°RSSCNNæ¨¡å‹
def evaluate_rsscnn(model, dataloader, return_preds=False):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for left_imgs, right_imgs, _, _, labels in dataloader:
            left_imgs = left_imgs.to(model.device)
            right_imgs = right_imgs.to(model.device)
            labels = labels.to(model.device)

            class_out, rank1, rank2 = model(left_imgs, right_imgs)
            loss = model.compute_loss(class_out, rank1, rank2, labels)

            total_loss += loss.item()
            _, predicted = torch.max(class_out.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            if return_preds:
                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(predicted.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total

    if return_preds:
        return avg_loss, accuracy, all_labels, all_preds
    return avg_loss, accuracy


def run_sscnn_training(cfg, dataset):
    """è¿è¡ŒSSCNNè®­ç»ƒ"""
    print("Training SSCNN with ...", cfg.base_model_name)
    early_stopper_sscnn = SchedulerEarlyStopper(max_plateaus=cfg.max_lr_plateaus)

    sscnn_model, sscnn_history, output_dir = train_sscnn(
        model_name=cfg.base_model_name,
        num_epochs=cfg.num_epochs,
        lr=cfg.learning_rate,
        batch_size=cfg.batch_size,
        early_stopper=early_stopper_sscnn,
        device=cfg.device,
        momentum=cfg.momentum,
        weight_decay=cfg.weight_decay,
        factor=cfg.factor,
        patience=cfg.patience,
        my_dataset=dataset
    )

    print(f"SSCNNè®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
    return sscnn_model, sscnn_history, output_dir


def run_rsscnn_training(cfg, dataset_1):
    """è¿è¡ŒRSSCNNè®­ç»ƒ"""
    print("\nTraining RSSCNN with", cfg.base_model_name)
    early_stopper_rsscnn = SchedulerEarlyStopper(max_plateaus=cfg.max_lr_plateaus)

    rsscnn_model, rsscnn_history, output_dir = train_rsscnn(
        model_name=cfg.base_model_name,
        num_epochs=cfg.num_epochs,
        lr=cfg.learning_rate,
        lambda_r=cfg.lambda_r,
        batch_size=cfg.batch_size,
        early_stopper=early_stopper_rsscnn,
        device=cfg.device,
        momentum=cfg.momentum,
        weight_decay=cfg.weight_decay,
        factor=cfg.factor,
        patience=cfg.patience,
        my_dataset_1=dataset_1
    )

    print(f"RSSCNNè®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
    return rsscnn_model, rsscnn_history, output_dir


if __name__ == "__main__":
    # åŠ è½½é…ç½®
    cfg = config.config_image_model.Config()

    # åˆ›å»ºæ•°æ®é›†
    dataset_1 = MyPP2Dataset(transform=cfg.transform, is_flipped=False)

    # é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹
    train_sscnn_flag = True  # è®¾ç½®ä¸ºTrueè®­ç»ƒSSCNNï¼ŒFalseåˆ™ä¸è®­ç»ƒ
    train_rsscnn_flag = False  # è®¾ç½®ä¸ºTrueè®­ç»ƒRSSCNNï¼ŒFalseåˆ™ä¸è®­ç»ƒ

    # è®­ç»ƒSSCNNæ¨¡å‹
    if train_sscnn_flag:
        sscnn_model, sscnn_history, sscnn_dir = run_sscnn_training(cfg, dataset_1)

    # è®­ç»ƒRSSCNNæ¨¡å‹
    if train_rsscnn_flag:
        rsscnn_model, rsscnn_history, rsscnn_dir = run_rsscnn_training(cfg, dataset_1)

    print("æ‰€æœ‰è®­ç»ƒä»»åŠ¡å®Œæˆï¼")